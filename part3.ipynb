{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled3.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOhsq9dBVF7lUcJvR5/I7/5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kkech/MachineLearningABCs/blob/part3/part3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lZx-Ot-7rB1b",
        "colab_type": "text"
      },
      "source": [
        "Intro to NN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-e4TcOeqWi4t",
        "colab_type": "text"
      },
      "source": [
        "Gradient Descent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sqk8wX4BWleP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "#Some helper functions for plotting and drawing lines\n",
        "\n",
        "def plot_points(X, y):\n",
        "    admitted = X[np.argwhere(y==1)]\n",
        "    rejected = X[np.argwhere(y==0)]\n",
        "    plt.scatter([s[0][0] for s in rejected], [s[0][1] for s in rejected], s = 25, color = 'blue', edgecolor = 'k')\n",
        "    plt.scatter([s[0][0] for s in admitted], [s[0][1] for s in admitted], s = 25, color = 'red', edgecolor = 'k')\n",
        "\n",
        "def display(m, b, color='g--'):\n",
        "    plt.xlim(-0.05,1.05)\n",
        "    plt.ylim(-0.05,1.05)\n",
        "    x = np.arange(-10, 10, 0.1)\n",
        "    plt.plot(x, m*x+b, color)\n",
        "\n",
        "data = pd.read_csv('data.csv', header=None)\n",
        "X = np.array(data[[0,1]])\n",
        "y = np.array(data[2])\n",
        "plot_points(X,y)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FnH4LZ-VWqck",
        "colab_type": "text"
      },
      "source": [
        "Implement the basic functions\n",
        "\n",
        "- Output (prediction) formula\n",
        "\n",
        "$$\\hat{y} = \\sigma(w_1 x_1 + w_2 x_2 + b)$$\n",
        "\n",
        "- Error function\n",
        "\n",
        "$$Error(y, \\hat{y}) = - y \\log(\\hat{y}) - (1-y) \\log(1-\\hat{y})$$\n",
        "\n",
        "- The function that updates the weights\n",
        "\n",
        "$$ w_i \\rightarrow w_i + \\alpha (y - \\hat{y}) x_i$$\n",
        "\n",
        "$$ b \\rightarrow b + \\alpha (y - \\hat{y})$$\n",
        "\n",
        "- Sigmoid activation function\n",
        "\n",
        "$$\\sigma(x) = \\frac{1}{1+e^{-x}}$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3PJBSKceXFwg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO: Implement the following functions\n",
        "\n",
        "# Output formula (prediction)\n",
        "def output_formula(features, weights, bias):\n",
        "    pass\n",
        "\n",
        "# Error formula (log-loss) \n",
        "def error_formula(y, output):\n",
        "    pass\n",
        "\n",
        "# Gradient descent step\n",
        "def update_weights(x, y, weights, bias, learnrate):\n",
        "    pass\n",
        "\n",
        "# Activation function (sigmoid)\n",
        "def sigmoid(x):\n",
        "    pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ow2ckuLXiE1",
        "colab_type": "text"
      },
      "source": [
        "Training function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pf3DyqlwXicN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#### DO NOT MODIFY ANYTHING BELOW THIS LINE\n",
        "np.random.seed(50)\n",
        "\n",
        "epochs = 100\n",
        "learnrate = 0.01\n",
        "\n",
        "def train(features, targets, epochs, learnrate, graph_lines=False):\n",
        "    \n",
        "    errors = []\n",
        "    n_records, n_features = features.shape\n",
        "    last_loss = None\n",
        "    weights = np.random.normal(scale=1 / n_features**.5, size=n_features)\n",
        "    bias = 0\n",
        "    for e in range(epochs):\n",
        "        del_w = np.zeros(weights.shape)\n",
        "        for x, y in zip(features, targets):\n",
        "            output = output_formula(x, weights, bias)\n",
        "            error = error_formula(y, output)\n",
        "            weights, bias = update_weights(x, y, weights, bias, learnrate)\n",
        "        \n",
        "        # Printing out the log-loss error on the training set\n",
        "        out = output_formula(features, weights, bias)\n",
        "        loss = np.mean(error_formula(targets, out))\n",
        "        errors.append(loss)\n",
        "        if e % (epochs / 10) == 0:\n",
        "            print(\"\\n========== Epoch\", e,\"==========\")\n",
        "            if last_loss and last_loss < loss:\n",
        "                print(\"Train loss: \", loss, \"  WARNING - Loss Increasing\")\n",
        "            else:\n",
        "                print(\"Train loss: \", loss)\n",
        "            last_loss = loss\n",
        "            predictions = out > 0.5\n",
        "            accuracy = np.mean(predictions == targets)\n",
        "            print(\"Accuracy: \", accuracy)\n",
        "        if graph_lines and e % (epochs / 100) == 0:\n",
        "            display(-weights[0]/weights[1], -bias/weights[1])\n",
        "            \n",
        "\n",
        "    # Plotting the solution boundary\n",
        "    plt.title(\"Solution boundary\")\n",
        "    display(-weights[0]/weights[1], -bias/weights[1], 'black')\n",
        "\n",
        "    # Plotting the data\n",
        "    plot_points(features, targets)\n",
        "    plt.show()\n",
        "\n",
        "    # Plotting the error\n",
        "    plt.title(\"Error Plot\")\n",
        "    plt.xlabel('Number of epochs')\n",
        "    plt.ylabel('Error')\n",
        "    plt.plot(errors)\n",
        "    plt.show()\n",
        "\n",
        "train(X, y, epochs, learnrate, True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YUOCl4OaYdAk",
        "colab_type": "text"
      },
      "source": [
        "Student Admissions NN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qmC-Zbf5Yp6M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Importing pandas and numpy\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Reading the csv file into a pandas DataFrame\n",
        "data = pd.read_csv('student_data.csv')\n",
        "\n",
        "# Printing out the first 10 rows of our data\n",
        "data[:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qAriiWE5YtnS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Importing matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "# Function to help us plot\n",
        "def plot_points(data):\n",
        "    X = np.array(data[[\"gre\",\"gpa\"]])\n",
        "    y = np.array(data[\"admit\"])\n",
        "    admitted = X[np.argwhere(y==1)]\n",
        "    rejected = X[np.argwhere(y==0)]\n",
        "    plt.scatter([s[0][0] for s in rejected], [s[0][1] for s in rejected], s = 25, color = 'red', edgecolor = 'k')\n",
        "    plt.scatter([s[0][0] for s in admitted], [s[0][1] for s in admitted], s = 25, color = 'cyan', edgecolor = 'k')\n",
        "    plt.xlabel('Test (GRE)')\n",
        "    plt.ylabel('Grades (GPA)')\n",
        "    \n",
        "# Plotting the points\n",
        "plot_points(data)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wFv7qHGhYu7o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Separating the ranks\n",
        "data_rank1 = data[data[\"rank\"]==1]\n",
        "data_rank2 = data[data[\"rank\"]==2]\n",
        "data_rank3 = data[data[\"rank\"]==3]\n",
        "data_rank4 = data[data[\"rank\"]==4]\n",
        "\n",
        "# Plotting the graphs\n",
        "plot_points(data_rank1)\n",
        "plt.title(\"Rank 1\")\n",
        "plt.show()\n",
        "plot_points(data_rank2)\n",
        "plt.title(\"Rank 2\")\n",
        "plt.show()\n",
        "plot_points(data_rank3)\n",
        "plt.title(\"Rank 3\")\n",
        "plt.show()\n",
        "plot_points(data_rank4)\n",
        "plt.title(\"Rank 4\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VwWAc5jhZAqg",
        "colab_type": "text"
      },
      "source": [
        "Implement the following\n",
        "\n",
        "* Make dummy variables (HINT: Use pandas Get Dummies)\n",
        "\n",
        "* Drop rank column\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zYHP_sf8ZCGg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO:  Make dummy variables for rank and concat existing columns\n",
        "one_hot_data = pass\n",
        "\n",
        "# TODO: Drop the previous rank column\n",
        "one_hot_data = pass\n",
        "\n",
        "# Print the first 10 rows of our data\n",
        "one_hot_data[:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "APRCVwJNZEuZ",
        "colab_type": "text"
      },
      "source": [
        "Implement the following\n",
        "\n",
        "* Scale the data.\n",
        "  * Normalize GPA in scale(0-1)\n",
        "    * Find GPA MAX\n",
        "    * Divide GPA column with GPA MAX\n",
        "  * Normalize GRE in scale(0-1)\n",
        "    * Find GRE MAX\n",
        "    * Divide GRE column with GRE MAX"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "14-EnR7MZFad",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Making a copy of our data\n",
        "processed_data = one_hot_data[:]\n",
        "\n",
        "# TODO: Scale the columns\n",
        "\n",
        "# Printing the first 10 rows of our procesed data\n",
        "processed_data[:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6_2OjTS4ZHUL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sample = np.random.choice(processed_data.index, size=int(len(processed_data)*0.9), replace=False)\n",
        "train_data, test_data = processed_data.iloc[sample], processed_data.drop(sample)\n",
        "\n",
        "print(\"Number of training samples is\", len(train_data))\n",
        "print(\"Number of testing samples is\", len(test_data))\n",
        "print(train_data[:10])\n",
        "print(test_data[:10])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RnkMB-5pZI80",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "features = train_data.drop('admit', axis=1)\n",
        "targets = train_data['admit']\n",
        "features_test = test_data.drop('admit', axis=1)\n",
        "targets_test = test_data['admit']\n",
        "\n",
        "print(features[:10])\n",
        "print(targets[:10])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zJa7iejdZKPd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Activation (sigmoid) function\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "def sigmoid_prime(x):\n",
        "    return sigmoid(x) * (1-sigmoid(x))\n",
        "def error_formula(y, output):\n",
        "    return - y*np.log(output) - (1 - y) * np.log(1-output)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y_vkU3rQkUz1",
        "colab_type": "text"
      },
      "source": [
        "Chain Rule\n",
        "\n",
        "* Forward propagation and evaluation\n",
        "\n",
        "$$x = \\alpha_1$$\n",
        "$$z_1 = w_1*x + b$$\n",
        "$$a_2 = \\sigma(z_1)$$\n",
        "\n",
        "* Find Cost\n",
        "  * MSE\n",
        "  * e.t.c.\n",
        "\n",
        "* Backpropagation -> aims to minimize the cost function by adjusting networkâ€™s weights and biases.\n",
        "  * Compute Gradients using ***chain rule.***\n",
        "  * Update weights and bias.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WWtwG8tqZNJ3",
        "colab_type": "text"
      },
      "source": [
        "Implement Error Term Formula\n",
        "\n",
        "$$ (y-\\hat{y}) \\sigma'(x) $$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EvuRBUw0ZOOy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO: Write the error term formula.\n",
        "def error_term_formula(x, y, output):\n",
        "    pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9gKEyy8CZQ1j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Neural Network hyperparameters\n",
        "epochs = 1000\n",
        "learnrate = 0.5\n",
        "\n",
        "# Training function\n",
        "def train_nn(features, targets, epochs, learnrate):\n",
        "    \n",
        "    # Use to same seed to make debugging easier\n",
        "    np.random.seed(50)\n",
        "\n",
        "    n_records, n_features = features.shape\n",
        "    last_loss = None\n",
        "\n",
        "    # Initialize weights\n",
        "    weights = np.random.normal(scale=1 / n_features**.5, size=n_features)\n",
        "\n",
        "    for e in range(epochs):\n",
        "        del_w = np.zeros(weights.shape)\n",
        "        for x, y in zip(features.values, targets):\n",
        "            # Loop through all records, x is the input, y is the target\n",
        "\n",
        "            # Activation of the output unit\n",
        "            #   Notice we multiply the inputs and the weights here \n",
        "            #   rather than storing h as a separate variable \n",
        "            output = sigmoid(np.dot(x, weights))\n",
        "\n",
        "            # The error, the target minus the network output\n",
        "            error = error_formula(y, output)\n",
        "\n",
        "            # The error term\n",
        "            error_term = error_term_formula(x, y, output)\n",
        "\n",
        "            # The gradient descent step, the error times the gradient times the inputs\n",
        "            del_w += error_term * x\n",
        "\n",
        "        # Update the weights here. The learning rate times the \n",
        "        # change in weights, divided by the number of records to average\n",
        "        weights += learnrate * del_w / n_records\n",
        "\n",
        "        # Printing out the mean square error on the training set\n",
        "        if e % (epochs / 10) == 0:\n",
        "            out = sigmoid(np.dot(features, weights))\n",
        "            loss = np.mean((out - targets) ** 2)\n",
        "            print(\"Epoch:\", e)\n",
        "            if last_loss and last_loss < loss:\n",
        "                print(\"Train loss: \", loss, \"  WARNING - Loss Increasing\")\n",
        "            else:\n",
        "                print(\"Train loss: \", loss)\n",
        "            last_loss = loss\n",
        "            print(\"=========\")\n",
        "    print(\"Finished training!\")\n",
        "    return weights\n",
        "    \n",
        "weights = train_nn(features, targets, epochs, learnrate)\n",
        "\n",
        "# Calculate accuracy on test data\n",
        "test_out = sigmoid(np.dot(features_test, weights))\n",
        "predictions = test_out > 0.5\n",
        "accuracy = np.mean(predictions == targets_test)\n",
        "print(\"Prediction accuracy: {:.3f}\".format(accuracy))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JEC0jVuSqqGQ",
        "colab_type": "text"
      },
      "source": [
        "Using Keras"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JsCoSslqqtYd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import keras\n",
        "\n",
        "sample = np.random.choice(processed_data.index, size=int(len(processed_data)*0.9), replace=False)\n",
        "train_data, test_data = processed_data.iloc[sample], processed_data.drop(sample)\n",
        "\n",
        "print(\"Number of training samples is\", len(train_data))\n",
        "print(\"Number of testing samples is\", len(test_data))\n",
        "print(train_data[:10])\n",
        "print(test_data[:10])\n",
        "\n",
        "# Separate data and one-hot encode the output\n",
        "# Note: We're also turning the data into numpy arrays, in order to train the model in Keras\n",
        "features = np.array(train_data.drop('admit', axis=1))\n",
        "targets = np.array(keras.utils.to_categorical(train_data['admit'], 2))\n",
        "features_test = np.array(test_data.drop('admit', axis=1))\n",
        "targets_test = np.array(keras.utils.to_categorical(test_data['admit'], 2))\n",
        "\n",
        "print(features[:10])\n",
        "print(targets[:10])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0sXZ299iquYL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Imports\n",
        "import numpy as np\n",
        "from keras.models import Sequential\n",
        "from keras.layers.core import Dense, Dropout, Activation\n",
        "from keras.optimizers import SGD\n",
        "from keras.utils import np_utils\n",
        "\n",
        "# Building the model\n",
        "model = Sequential()\n",
        "model.add(Dense(256, activation='relu', input_shape=(6,)))\n",
        "model.add(Dropout(.2))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dropout(.2))\n",
        "model.add(Dense(2, activation='softmax'))\n",
        "\n",
        "# Compiling the model\n",
        "model.compile(loss = 'categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-R-AkKhIq2JD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Training the model\n",
        "# Running and evaluating the model\n",
        "model.fit(features, targets, epochs=1000, batch_size=128, verbose=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LDegxM1xq2tF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Evaluating the model on the training and testing set\n",
        "score = model.evaluate(features, targets)\n",
        "print(\"\\n Training Accuracy:\", score[1])\n",
        "score = model.evaluate(features_test, targets_test)\n",
        "print(\"\\n Testing Accuracy:\", score[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VexyrkHb3UIt",
        "colab_type": "text"
      },
      "source": [
        "Analyzing IMDB Data in Keras"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cl_hbbki3Tq-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Imports\n",
        "import numpy as np\n",
        "import keras\n",
        "from keras.datasets import imdb\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "np.random.seed(50)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8bJxYbN836hA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Loading the data (it's preloaded in Keras)\n",
        "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=1000)\n",
        "\n",
        "print(x_train.shape)\n",
        "print(x_test.shape)\n",
        "\n",
        "print(x_train[0])\n",
        "print(y_train[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q4p1yC374A1J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# One-hot encoding the output into vector mode, each of length 1000\n",
        "tokenizer = Tokenizer(num_words=1000)\n",
        "x_train = tokenizer.sequences_to_matrix(x_train, mode='binary')\n",
        "x_test = tokenizer.sequences_to_matrix(x_test, mode='binary')\n",
        "# print(x_train[0])\n",
        "\n",
        "# One-hot encoding the output\n",
        "num_classes = 2\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
        "print(y_train.shape)\n",
        "print(y_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UUbk0Fgq4KjS",
        "colab_type": "text"
      },
      "source": [
        "Implement the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tBujTemM36tu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qm1lEjdB4O2I",
        "colab_type": "text"
      },
      "source": [
        "Train the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JCwnCt-i362z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V01ga9FD4ViV",
        "colab_type": "text"
      },
      "source": [
        "Evaluate the model. Try accuracy over 80%"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L0VVUQOo4IBl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "score = model.evaluate(x_test, y_test, verbose=0)\n",
        "print(\"Accuracy: \", score[1])"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}