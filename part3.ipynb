{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled3.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMp2TuHwDZ0i5DGz9sKS71P",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kkech/MachineLearningABCs/blob/part3/part3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lZx-Ot-7rB1b",
        "colab_type": "text"
      },
      "source": [
        "Intro to NN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-e4TcOeqWi4t",
        "colab_type": "text"
      },
      "source": [
        "Gradient Descent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sqk8wX4BWleP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "#Some helper functions for plotting and drawing lines\n",
        "\n",
        "def plot_points(X, y):\n",
        "    admitted = X[np.argwhere(y==1)]\n",
        "    rejected = X[np.argwhere(y==0)]\n",
        "    plt.scatter([s[0][0] for s in rejected], [s[0][1] for s in rejected], s = 25, color = 'blue', edgecolor = 'k')\n",
        "    plt.scatter([s[0][0] for s in admitted], [s[0][1] for s in admitted], s = 25, color = 'red', edgecolor = 'k')\n",
        "\n",
        "def display(m, b, color='g--'):\n",
        "    plt.xlim(-0.05,1.05)\n",
        "    plt.ylim(-0.05,1.05)\n",
        "    x = np.arange(-10, 10, 0.1)\n",
        "    plt.plot(x, m*x+b, color)\n",
        "\n",
        "data = pd.read_csv('data.csv', header=None)\n",
        "X = np.array(data[[0,1]])\n",
        "y = np.array(data[2])\n",
        "plot_points(X,y)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FnH4LZ-VWqck",
        "colab_type": "text"
      },
      "source": [
        "Implement the basic functions\n",
        "\n",
        "- Output (prediction) formula\n",
        "\n",
        "$$\\hat{y} = \\sigma(w_1 x_1 + w_2 x_2 + b)$$\n",
        "\n",
        "- Error function\n",
        "\n",
        "$$Error(y, \\hat{y}) = - y \\log(\\hat{y}) - (1-y) \\log(1-\\hat{y})$$\n",
        "\n",
        "- The function that updates the weights\n",
        "\n",
        "$$ w_i \\rightarrow w_i + \\alpha (y - \\hat{y}) x_i$$\n",
        "\n",
        "$$ b \\rightarrow b + \\alpha (y - \\hat{y})$$\n",
        "\n",
        "- Sigmoid activation function\n",
        "\n",
        "$$\\sigma(x) = \\frac{1}{1+e^{-x}}$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3PJBSKceXFwg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO: Implement the following functions\n",
        "\n",
        "# Output formula (prediction)\n",
        "def output_formula(features, weights, bias):\n",
        "    pass\n",
        "\n",
        "# Error formula (log-loss) \n",
        "def error_formula(y, output):\n",
        "    pass\n",
        "\n",
        "# Gradient descent step\n",
        "def update_weights(x, y, weights, bias, learnrate):\n",
        "    pass\n",
        "\n",
        "# Activation function (sigmoid)\n",
        "def sigmoid(x):\n",
        "    pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ow2ckuLXiE1",
        "colab_type": "text"
      },
      "source": [
        "Training function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pf3DyqlwXicN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#### DO NOT MODIFY ANYTHING BELOW THIS LINE\n",
        "np.random.seed(50)\n",
        "\n",
        "epochs = 100\n",
        "learnrate = 0.01\n",
        "\n",
        "def train(features, targets, epochs, learnrate, graph_lines=False):\n",
        "    \n",
        "    errors = []\n",
        "    n_records, n_features = features.shape\n",
        "    last_loss = None\n",
        "    weights = np.random.normal(scale=1 / n_features**.5, size=n_features)\n",
        "    bias = 0\n",
        "    for e in range(epochs):\n",
        "        del_w = np.zeros(weights.shape)\n",
        "        for x, y in zip(features, targets):\n",
        "            output = output_formula(x, weights, bias)\n",
        "            error = error_formula(y, output)\n",
        "            weights, bias = update_weights(x, y, weights, bias, learnrate)\n",
        "        \n",
        "        # Printing out the log-loss error on the training set\n",
        "        out = output_formula(features, weights, bias)\n",
        "        loss = np.mean(error_formula(targets, out))\n",
        "        errors.append(loss)\n",
        "        if e % (epochs / 10) == 0:\n",
        "            print(\"\\n========== Epoch\", e,\"==========\")\n",
        "            if last_loss and last_loss < loss:\n",
        "                print(\"Train loss: \", loss, \"  WARNING - Loss Increasing\")\n",
        "            else:\n",
        "                print(\"Train loss: \", loss)\n",
        "            last_loss = loss\n",
        "            predictions = out > 0.5\n",
        "            accuracy = np.mean(predictions == targets)\n",
        "            print(\"Accuracy: \", accuracy)\n",
        "        if graph_lines and e % (epochs / 100) == 0:\n",
        "            display(-weights[0]/weights[1], -bias/weights[1])\n",
        "            \n",
        "\n",
        "    # Plotting the solution boundary\n",
        "    plt.title(\"Solution boundary\")\n",
        "    display(-weights[0]/weights[1], -bias/weights[1], 'black')\n",
        "\n",
        "    # Plotting the data\n",
        "    plot_points(features, targets)\n",
        "    plt.show()\n",
        "\n",
        "    # Plotting the error\n",
        "    plt.title(\"Error Plot\")\n",
        "    plt.xlabel('Number of epochs')\n",
        "    plt.ylabel('Error')\n",
        "    plt.plot(errors)\n",
        "    plt.show()\n",
        "\n",
        "train(X, y, epochs, learnrate, True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YUOCl4OaYdAk",
        "colab_type": "text"
      },
      "source": [
        "Student Admissions NN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qmC-Zbf5Yp6M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Importing pandas and numpy\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Reading the csv file into a pandas DataFrame\n",
        "data = pd.read_csv('student_data.csv')\n",
        "\n",
        "# Printing out the first 10 rows of our data\n",
        "data[:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qAriiWE5YtnS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Importing matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "# Function to help us plot\n",
        "def plot_points(data):\n",
        "    X = np.array(data[[\"gre\",\"gpa\"]])\n",
        "    y = np.array(data[\"admit\"])\n",
        "    admitted = X[np.argwhere(y==1)]\n",
        "    rejected = X[np.argwhere(y==0)]\n",
        "    plt.scatter([s[0][0] for s in rejected], [s[0][1] for s in rejected], s = 25, color = 'red', edgecolor = 'k')\n",
        "    plt.scatter([s[0][0] for s in admitted], [s[0][1] for s in admitted], s = 25, color = 'cyan', edgecolor = 'k')\n",
        "    plt.xlabel('Test (GRE)')\n",
        "    plt.ylabel('Grades (GPA)')\n",
        "    \n",
        "# Plotting the points\n",
        "plot_points(data)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wFv7qHGhYu7o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Separating the ranks\n",
        "data_rank1 = data[data[\"rank\"]==1]\n",
        "data_rank2 = data[data[\"rank\"]==2]\n",
        "data_rank3 = data[data[\"rank\"]==3]\n",
        "data_rank4 = data[data[\"rank\"]==4]\n",
        "\n",
        "# Plotting the graphs\n",
        "plot_points(data_rank1)\n",
        "plt.title(\"Rank 1\")\n",
        "plt.show()\n",
        "plot_points(data_rank2)\n",
        "plt.title(\"Rank 2\")\n",
        "plt.show()\n",
        "plot_points(data_rank3)\n",
        "plt.title(\"Rank 3\")\n",
        "plt.show()\n",
        "plot_points(data_rank4)\n",
        "plt.title(\"Rank 4\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VwWAc5jhZAqg",
        "colab_type": "text"
      },
      "source": [
        "Implement the following\n",
        "\n",
        "* Make dummy variables (HINT: Use pandas Get Dummies)\n",
        "\n",
        "* Drop rank column\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zYHP_sf8ZCGg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO:  Make dummy variables for rank and concat existing columns\n",
        "one_hot_data = pass\n",
        "\n",
        "# TODO: Drop the previous rank column\n",
        "one_hot_data = pass\n",
        "\n",
        "# Print the first 10 rows of our data\n",
        "one_hot_data[:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "APRCVwJNZEuZ",
        "colab_type": "text"
      },
      "source": [
        "Implement the following\n",
        "\n",
        "* Scale the data.\n",
        "  * Normalize GPA in scale(0-1)\n",
        "    * Find GPA MAX\n",
        "    * Divide GPA column with GPA MAX\n",
        "  * Normalize GRE in scale(0-1)\n",
        "    * Find GRE MAX\n",
        "    * Divide GRE column with GRE MAX"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "14-EnR7MZFad",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Making a copy of our data\n",
        "processed_data = one_hot_data[:]\n",
        "\n",
        "# TODO: Scale the columns\n",
        "\n",
        "# Printing the first 10 rows of our procesed data\n",
        "processed_data[:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6_2OjTS4ZHUL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sample = np.random.choice(processed_data.index, size=int(len(processed_data)*0.9), replace=False)\n",
        "train_data, test_data = processed_data.iloc[sample], processed_data.drop(sample)\n",
        "\n",
        "print(\"Number of training samples is\", len(train_data))\n",
        "print(\"Number of testing samples is\", len(test_data))\n",
        "print(train_data[:10])\n",
        "print(test_data[:10])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RnkMB-5pZI80",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "features = train_data.drop('admit', axis=1)\n",
        "targets = train_data['admit']\n",
        "features_test = test_data.drop('admit', axis=1)\n",
        "targets_test = test_data['admit']\n",
        "\n",
        "print(features[:10])\n",
        "print(targets[:10])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zJa7iejdZKPd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Activation (sigmoid) function\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "def sigmoid_prime(x):\n",
        "    return sigmoid(x) * (1-sigmoid(x))\n",
        "def error_formula(y, output):\n",
        "    return - y*np.log(output) - (1 - y) * np.log(1-output)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y_vkU3rQkUz1",
        "colab_type": "text"
      },
      "source": [
        "Chain Rule\n",
        "\n",
        "* Forward propagation and evaluation\n",
        "\n",
        "$$x = \\alpha_1$$\n",
        "$$z_1 = w_1*x + b$$\n",
        "$$a_2 = \\sigma(z_1)$$\n",
        "\n",
        "* Find Cost\n",
        "  * MSE\n",
        "  * e.t.c.\n",
        "\n",
        "* Backpropagation -> aims to minimize the cost function by adjusting networkâ€™s weights and biases.\n",
        "  * Compute Gradients using ***chain rule.***\n",
        "  * Update weights and bias.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WWtwG8tqZNJ3",
        "colab_type": "text"
      },
      "source": [
        "Implement Error Term Formula\n",
        "\n",
        "$$ (y-\\hat{y}) \\sigma'(x) $$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EvuRBUw0ZOOy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO: Write the error term formula.\n",
        "def error_term_formula(x, y, output):\n",
        "    pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9gKEyy8CZQ1j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Neural Network hyperparameters\n",
        "epochs = 1000\n",
        "learnrate = 0.5\n",
        "\n",
        "# Training function\n",
        "def train_nn(features, targets, epochs, learnrate):\n",
        "    \n",
        "    # Use to same seed to make debugging easier\n",
        "    np.random.seed(50)\n",
        "\n",
        "    n_records, n_features = features.shape\n",
        "    last_loss = None\n",
        "\n",
        "    # Initialize weights\n",
        "    weights = np.random.normal(scale=1 / n_features**.5, size=n_features)\n",
        "\n",
        "    for e in range(epochs):\n",
        "        del_w = np.zeros(weights.shape)\n",
        "        for x, y in zip(features.values, targets):\n",
        "            # Loop through all records, x is the input, y is the target\n",
        "\n",
        "            # Activation of the output unit\n",
        "            #   Notice we multiply the inputs and the weights here \n",
        "            #   rather than storing h as a separate variable \n",
        "            output = sigmoid(np.dot(x, weights))\n",
        "\n",
        "            # The error, the target minus the network output\n",
        "            error = error_formula(y, output)\n",
        "\n",
        "            # The error term\n",
        "            error_term = error_term_formula(x, y, output)\n",
        "\n",
        "            # The gradient descent step, the error times the gradient times the inputs\n",
        "            del_w += error_term * x\n",
        "\n",
        "        # Update the weights here. The learning rate times the \n",
        "        # change in weights, divided by the number of records to average\n",
        "        weights += learnrate * del_w / n_records\n",
        "\n",
        "        # Printing out the mean square error on the training set\n",
        "        if e % (epochs / 10) == 0:\n",
        "            out = sigmoid(np.dot(features, weights))\n",
        "            loss = np.mean((out - targets) ** 2)\n",
        "            print(\"Epoch:\", e)\n",
        "            if last_loss and last_loss < loss:\n",
        "                print(\"Train loss: \", loss, \"  WARNING - Loss Increasing\")\n",
        "            else:\n",
        "                print(\"Train loss: \", loss)\n",
        "            last_loss = loss\n",
        "            print(\"=========\")\n",
        "    print(\"Finished training!\")\n",
        "    return weights\n",
        "    \n",
        "weights = train_nn(features, targets, epochs, learnrate)\n",
        "\n",
        "# Calculate accuracy on test data\n",
        "test_out = sigmoid(np.dot(features_test, weights))\n",
        "predictions = test_out > 0.5\n",
        "accuracy = np.mean(predictions == targets_test)\n",
        "print(\"Prediction accuracy: {:.3f}\".format(accuracy))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JEC0jVuSqqGQ",
        "colab_type": "text"
      },
      "source": [
        "Using Keras"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JsCoSslqqtYd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import keras\n",
        "\n",
        "sample = np.random.choice(processed_data.index, size=int(len(processed_data)*0.9), replace=False)\n",
        "train_data, test_data = processed_data.iloc[sample], processed_data.drop(sample)\n",
        "\n",
        "print(\"Number of training samples is\", len(train_data))\n",
        "print(\"Number of testing samples is\", len(test_data))\n",
        "print(train_data[:10])\n",
        "print(test_data[:10])\n",
        "\n",
        "# Separate data and one-hot encode the output\n",
        "# Note: We're also turning the data into numpy arrays, in order to train the model in Keras\n",
        "features = np.array(train_data.drop('admit', axis=1))\n",
        "targets = np.array(keras.utils.to_categorical(train_data['admit'], 2))\n",
        "features_test = np.array(test_data.drop('admit', axis=1))\n",
        "targets_test = np.array(keras.utils.to_categorical(test_data['admit'], 2))\n",
        "\n",
        "print(features[:10])\n",
        "print(targets[:10])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0sXZ299iquYL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Imports\n",
        "import numpy as np\n",
        "from keras.models import Sequential\n",
        "from keras.layers.core import Dense, Dropout, Activation\n",
        "from keras.optimizers import SGD\n",
        "from keras.utils import np_utils\n",
        "\n",
        "# Building the model\n",
        "model = Sequential()\n",
        "model.add(Dense(256, activation='relu', input_shape=(6,)))\n",
        "model.add(Dropout(.2))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dropout(.2))\n",
        "model.add(Dense(2, activation='softmax'))\n",
        "\n",
        "# Compiling the model\n",
        "model.compile(loss = 'categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-R-AkKhIq2JD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Training the model\n",
        "# Running and evaluating the model\n",
        "model.fit(features, targets, epochs=1000, batch_size=128, verbose=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LDegxM1xq2tF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Evaluating the model on the training and testing set\n",
        "score = model.evaluate(features, targets)\n",
        "print(\"\\n Training Accuracy:\", score[1])\n",
        "score = model.evaluate(features_test, targets_test)\n",
        "print(\"\\n Testing Accuracy:\", score[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VexyrkHb3UIt",
        "colab_type": "text"
      },
      "source": [
        "Analyzing IMDB Data in Keras"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cl_hbbki3Tq-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Imports\n",
        "import numpy as np\n",
        "import keras\n",
        "from keras.datasets import imdb\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "np.random.seed(50)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8bJxYbN836hA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Loading the data (it's preloaded in Keras)\n",
        "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=1000)\n",
        "\n",
        "print(x_train.shape)\n",
        "print(x_test.shape)\n",
        "\n",
        "print(x_train[0])\n",
        "print(y_train[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q4p1yC374A1J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# One-hot encoding the output into vector mode, each of length 1000\n",
        "tokenizer = Tokenizer(num_words=1000)\n",
        "x_train = tokenizer.sequences_to_matrix(x_train, mode='binary')\n",
        "x_test = tokenizer.sequences_to_matrix(x_test, mode='binary')\n",
        "# print(x_train[0])\n",
        "\n",
        "# One-hot encoding the output\n",
        "num_classes = 2\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
        "print(y_train.shape)\n",
        "print(y_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UUbk0Fgq4KjS",
        "colab_type": "text"
      },
      "source": [
        "Implement the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tBujTemM36tu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qm1lEjdB4O2I",
        "colab_type": "text"
      },
      "source": [
        "Train the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JCwnCt-i362z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V01ga9FD4ViV",
        "colab_type": "text"
      },
      "source": [
        "Evaluate the model. Try accuracy over 80%"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L0VVUQOo4IBl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "score = model.evaluate(x_test, y_test, verbose=0)\n",
        "print(\"Accuracy: \", score[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M5nHnQ0eAHPC",
        "colab_type": "text"
      },
      "source": [
        "Create your ML artwork"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YOqIQw1aAMmX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from io import BytesIO\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "from torchvision import transforms, models\n",
        "import requests\n",
        "%matplotlib inline\n",
        "\n",
        "\n",
        "# get the \"features\" portion of VGG19 (we will not need the \"classifier\" portion)\n",
        "vgg = models.vgg19(pretrained=True).features\n",
        "\n",
        "# freeze all VGG parameters since we're only optimizing the target image\n",
        "for param in vgg.parameters():\n",
        "    param.requires_grad_(False)\n",
        "# move the model to GPU, if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "vgg.to(device)\n",
        "\n",
        "def load_image(img_path, max_size=400, shape=None):\n",
        "    ''' Load in and transform an image, making sure the image\n",
        "       is <= 400 pixels in the x-y dims.'''\n",
        "    if \"http\" in img_path:\n",
        "        response = requests.get(img_path)\n",
        "        image = Image.open(BytesIO(response.content)).convert('RGB')\n",
        "    else:\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "    \n",
        "    # large images will slow down processing\n",
        "    if max(image.size) > max_size:\n",
        "        size = max_size\n",
        "    else:\n",
        "        size = max(image.size)\n",
        "    \n",
        "    if shape is not None:\n",
        "        size = shape\n",
        "        \n",
        "    in_transform = transforms.Compose([\n",
        "                        transforms.Resize(size),\n",
        "                        transforms.ToTensor(),\n",
        "                        transforms.Normalize((0.485, 0.456, 0.406), \n",
        "                                             (0.229, 0.224, 0.225))])\n",
        "\n",
        "    # discard the transparent, alpha channel (that's the :3) and add the batch dimension\n",
        "    image = in_transform(image)[:3,:,:].unsqueeze(0)\n",
        "    \n",
        "    return image\n",
        "\n",
        "# load in content and style image\n",
        "content = load_image('content.jpg').to(device)\n",
        "# Resize style to match content, makes code easier\n",
        "style = load_image('style.jpg', shape=content.shape[-2:]).to(device)\n",
        "\n",
        "# helper function for un-normalizing an image \n",
        "# and converting it from a Tensor image to a NumPy image for display\n",
        "def im_convert(tensor):\n",
        "    \"\"\" Display a tensor as an image. \"\"\"\n",
        "    \n",
        "    image = tensor.to(\"cpu\").clone().detach()\n",
        "    image = image.numpy().squeeze()\n",
        "    image = image.transpose(1,2,0)\n",
        "    image = image * np.array((0.229, 0.224, 0.225)) + np.array((0.485, 0.456, 0.406))\n",
        "    image = image.clip(0, 1)\n",
        "\n",
        "    return image\n",
        "\n",
        "def get_features(image, model, layers=None):\n",
        "    \"\"\" Run an image forward through a model and get the features for \n",
        "        a set of layers. Default layers are for VGGNet matching Gatys et al (2016)\n",
        "    \"\"\"\n",
        "    \n",
        "    ## TODO: Complete mapping layer names of PyTorch's VGGNet to names from the paper\n",
        "    ## Need the layers for the content and style representations of an image\n",
        "    if layers is None:\n",
        "        layers = {'0': 'conv1_1',\n",
        "                  '5': 'conv2_1', \n",
        "                  '10': 'conv3_1', \n",
        "                  '19': 'conv4_1',\n",
        "                  '21': 'conv4_2',  ## content representation\n",
        "                  '28': 'conv5_1'}\n",
        "        \n",
        "    features = {}\n",
        "    x = image\n",
        "    # model._modules is a dictionary holding each module in the model\n",
        "    for name, layer in model._modules.items():\n",
        "        x = layer(x)\n",
        "        if name in layers:\n",
        "            features[layers[name]] = x\n",
        "            \n",
        "    return features\n",
        "\n",
        "def gram_matrix(tensor):\n",
        "    \"\"\" Calculate the Gram Matrix of a given tensor \n",
        "        Gram Matrix: https://en.wikipedia.org/wiki/Gramian_matrix\n",
        "    \"\"\"\n",
        "    \n",
        "    # get the batch_size, depth, height, and width of the Tensor\n",
        "    _, d, h, w = tensor.size()\n",
        "    \n",
        "    # reshape so we're multiplying the features for each channel\n",
        "    tensor = tensor.view(d, h * w)\n",
        "    \n",
        "    # calculate the gram matrix\n",
        "    gram = torch.mm(tensor, tensor.t())\n",
        "    \n",
        "    return gram \n",
        "  \n",
        "# get content and style features only once before training\n",
        "content_features = get_features(content, vgg)\n",
        "style_features = get_features(style, vgg)\n",
        "\n",
        "# calculate the gram matrices for each layer of our style representation\n",
        "style_grams = {layer: gram_matrix(style_features[layer]) for layer in style_features}\n",
        "\n",
        "# create a third \"target\" image and prep it for change\n",
        "# it is a good idea to start off with the target as a copy of our *content* image\n",
        "# then iteratively change its style\n",
        "target = content.clone().requires_grad_(True).to(device)\n",
        "\n",
        "# weights for each style layer \n",
        "# weighting earlier layers more will result in *larger* style artifacts\n",
        "# notice we are excluding `conv4_2` our content representation\n",
        "style_weights = {'conv1_1': 1.,\n",
        "                 'conv2_1': 0.75,\n",
        "                 'conv3_1': 0.2,\n",
        "                 'conv4_1': 0.2,\n",
        "                 'conv5_1': 0.2}\n",
        "\n",
        "content_weight = 1  # alpha\n",
        "style_weight = 1e6  # beta\n",
        "\n",
        "# for displaying the target image, intermittently\n",
        "show_every = 400\n",
        "\n",
        "# iteration hyperparameters\n",
        "optimizer = optim.Adam([target], lr=0.003)\n",
        "steps = 2000  # decide how many iterations to update your image (5000)\n",
        "\n",
        "for ii in range(1, steps+1):\n",
        "    \n",
        "    # get the features from your target image\n",
        "    target_features = get_features(target, vgg)\n",
        "    \n",
        "    # the content loss\n",
        "    content_loss = torch.mean((target_features['conv4_2'] - content_features['conv4_2'])**2)\n",
        "    \n",
        "    # the style loss\n",
        "    # initialize the style loss to 0\n",
        "    style_loss = 0\n",
        "    # then add to it for each layer's gram matrix loss\n",
        "    for layer in style_weights:\n",
        "        # get the \"target\" style representation for the layer\n",
        "        target_feature = target_features[layer]\n",
        "        target_gram = gram_matrix(target_feature)\n",
        "        _, d, h, w = target_feature.shape\n",
        "        # get the \"style\" style representation\n",
        "        style_gram = style_grams[layer]\n",
        "        # the style loss for one layer, weighted appropriately\n",
        "        layer_style_loss = style_weights[layer] * torch.mean((target_gram - style_gram)**2)\n",
        "        # add to the style loss\n",
        "        style_loss += layer_style_loss / (d * h * w)\n",
        "        \n",
        "    # calculate the *total* loss\n",
        "    total_loss = content_weight * content_loss + style_weight * style_loss\n",
        "    \n",
        "    # update your target image\n",
        "    optimizer.zero_grad()\n",
        "    total_loss.backward()\n",
        "    optimizer.step()\n",
        "    \n",
        "    # display intermediate images and print the loss\n",
        "    if  ii % show_every == 0:\n",
        "        print('Total loss: ', total_loss.item())\n",
        "        plt.imshow(im_convert(target))\n",
        "        plt.show()\n",
        "\n",
        "# display content and final, target image\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 10))\n",
        "ax1.imshow(im_convert(content))\n",
        "ax2.imshow(im_convert(target))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r-uo3IPsFMyQ",
        "colab_type": "text"
      },
      "source": [
        "Save your artwork. You can upload it in our FB team."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B3jyVFsvAbV-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import files\n",
        "\n",
        "# display content and final, target image\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 10))\n",
        "ax1.imshow(im_convert(content))\n",
        "ax2.imshow(im_convert(target))\n",
        "\n",
        "fig.savefig('genImg.png')\n",
        "files.download('genImg.png')\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}